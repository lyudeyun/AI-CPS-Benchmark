
<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN">
<html xmlns:mwsh="http://www.mathworks.com/namespace/mcode/v1/syntaxhighlight.dtd">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   
      <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      -->
      <title>demo_multivariable_calc</title>
      <meta name="generator" content="MATLAB 7.4">
      <meta name="date" content="2007-11-06">
      <meta name="m-file" content="demo_multivariable_calc"><style>

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head>
   <body>
      <div class="content">
         <h2>Contents</h2>
         <div>
            <ul>
               <li><a href="#2">Gradient of the Rosenbrock function at [1,1], the global minimizer</a></li>
               <li><a href="#3">The Hessian matrix at the minimizer should be positive definite</a></li>
               <li><a href="#4">Gradient estimation using gradest - a function of 5 variables</a></li>
               <li><a href="#5">Simple Hessian matrix of a problem with 3 independent variables</a></li>
               <li><a href="#6">A semi-definite Hessian matrix</a></li>
               <li><a href="#7">Directional derivative of the Rosenbrock function at the solution</a></li>
               <li><a href="#8">Directional derivative at other locations</a></li>
               <li><a href="#9">Jacobian matrix of a scalar function is just the gradient</a></li>
               <li><a href="#10">Jacobian matrix of a linear system will reduce to the design matrix</a></li>
               <li><a href="#11">The jacobian matrix of a nonlinear transformation of variables</a></li>
            </ul>
         </div><pre class="codeinput"><span class="comment">% Multivariate calculus demo script</span>

<span class="comment">% This script file is designed to be used in cell mode</span>
<span class="comment">% from the matlab editor, or best of all, use the publish</span>
<span class="comment">% to HTML feature from the matlab editor. Older versions</span>
<span class="comment">% of matlab can copy and paste entire blocks of code into</span>
<span class="comment">% the Matlab command window.</span>

<span class="comment">% Typical usage of the gradient and Hessian might be in</span>
<span class="comment">% optimization problems, where one might compare an analytically</span>
<span class="comment">% derived gradient for correctness, or use the Hessian matrix</span>
<span class="comment">% to compute confidence interval estimates on parameters in a</span>
<span class="comment">% maximum likelihood estimation.</span>
</pre><h2>Gradient of the Rosenbrock function at [1,1], the global minimizer<a name="2"></a></h2><pre class="codeinput">rosen = @(x) (1-x(1)).^2 + 105*(x(2)-x(1).^2).^2;
<span class="comment">% The gradient should be zero (within floating point noise)</span>
[grad,err] = gradest(rosen,[1 1])
</pre><pre class="codeoutput">
grad =

     0     0


err =

  1.0e-017 *

    0.1896         0

</pre><h2>The Hessian matrix at the minimizer should be positive definite<a name="3"></a></h2><pre class="codeinput">H = hessian(rosen,[1 1])
<span class="comment">% The eigenvalues of h should be positive</span>
eig(H)
</pre><pre class="codeoutput">
H =

  842.0000 -420.0000
 -420.0000  210.0000


ans =

  1.0e+003 *

    0.0004
    1.0516

</pre><h2>Gradient estimation using gradest - a function of 5 variables<a name="4"></a></h2><pre class="codeinput">[grad,err] = gradest(@(x) sum(x.^2),[1 2 3 4 5])
</pre><pre class="codeoutput">
grad =

    2.0000    4.0000    6.0000    8.0000   10.0000


err =

  1.0e-013 *

    0.0434    0.0868    0.1418    0.1737    0.2005

</pre><h2>Simple Hessian matrix of a problem with 3 independent variables<a name="5"></a></h2><pre class="codeinput">[H,err] = hessian(@(x) x(1) + x(2)^2 + x(3)^3,[1 2 3])
</pre><pre class="codeoutput">
H =

         0         0         0
         0    2.0000         0
         0         0   18.0000


err =

  1.0e-013 *

         0         0         0
         0    0.0751         0
         0         0    0.6664

</pre><h2>A semi-definite Hessian matrix<a name="6"></a></h2><pre class="codeinput">H = hessian(@(xy) cos(xy(1) - xy(2)),[0 0])
<span class="comment">% one of these eigenvalues will be zero (approximately)</span>
eig(H)
</pre><pre class="codeoutput">
H =

   -1.0000    1.0000
    1.0000   -1.0000


ans =

   -2.0000
   -0.0000

</pre><h2>Directional derivative of the Rosenbrock function at the solution<a name="7"></a></h2>
         <p>This should be zero. Ok, its a trivial test case.</p><pre class="codeinput">[dd,err] = directionaldiff(rosen,[1 1],[1 2])
</pre><pre class="codeoutput">
dd =

     0


err =

     0

</pre><h2>Directional derivative at other locations<a name="8"></a></h2><pre class="codeinput">[dd,err] = directionaldiff(rosen,[2 3],[1 -1])

<span class="comment">% We can test this example</span>
v = [1 -1];
v = v/norm(v);
g = gradest(rosen,[2 3]);

<span class="comment">% The directional derivative will be the dot product of the gradient with</span>
<span class="comment">% the (unit normalized) vector. So this difference will be (approx) zero.</span>
dot(g,v) - dd
</pre><pre class="codeoutput">
dd =

  743.8763


err =

  1.8467e-012


ans =

  1.5916e-012

</pre><h2>Jacobian matrix of a scalar function is just the gradient<a name="9"></a></h2><pre class="codeinput">[jac,err] = jacobianest(rosen,[2 3])

grad = gradest(rosen,[2 3])
</pre><pre class="codeoutput">
jac =

   842  -210


err =

  1.0e-011 *

    0.1283         0


grad =

   842  -210

</pre><h2>Jacobian matrix of a linear system will reduce to the design matrix<a name="10"></a></h2><pre class="codeinput">A = rand(5,3);
b = rand(5,1);
fun = @(x) (A*x-b);

x = rand(3,1);
[jac,err] = jacobianest(fun,x)

disp <span class="string">'This should be essentially zero at any location x'</span>
jac - A
</pre><pre class="codeoutput">
jac =

    0.8147    0.0975    0.1576
    0.9058    0.2785    0.9706
    0.1270    0.5469    0.9572
    0.9134    0.9575    0.4854
    0.6324    0.9649    0.8003


err =

  1.0e-014 *

    0.1772    0.0157    0.0313
         0    0.1535    0.1772
    0.0443         0    0.1253
    0.1772    0.2171    0.0627
    0.1253    0.1253    0.1253

This should be essentially zero at any location x

ans =

  1.0e-015 *

         0   -0.3331   -0.0278
   -0.2220    0.2220         0
         0   -0.1110   -0.1110
   -0.2220         0   -0.1665
    0.4441    0.1110    0.1110

</pre><h2>The jacobian matrix of a nonlinear transformation of variables<a name="11"></a></h2>
         <p>evaluated at some arbitrary location [-2, -3]</p><pre class="codeinput">fun = @(xy) [xy(1).^2, cos(xy(1) - xy(2))];
[jac,err] = jacobianest(fun,[-2 -3])
</pre><pre class="codeoutput">
jac =

   -4.0000         0
   -0.8415    0.8415


err =

  1.0e-013 *

    0.0868         0
    0.6102    0.1997

</pre><p class="footer"><br>
            Published with MATLAB&reg; 7.4<br></p>
      </div>
      <!--
##### SOURCE BEGIN #####
% Multivariate calculus demo script

% This script file is designed to be used in cell mode
% from the matlab editor, or best of all, use the publish
% to HTML feature from the matlab editor. Older versions
% of matlab can copy and paste entire blocks of code into
% the Matlab command window.

% Typical usage of the gradient and Hessian might be in
% optimization problems, where one might compare an analytically
% derived gradient for correctness, or use the Hessian matrix
% to compute confidence interval estimates on parameters in a
% maximum likelihood estimation.

%% Gradient of the Rosenbrock function at [1,1], the global minimizer
rosen = @(x) (1-x(1)).^2 + 105*(x(2)-x(1).^2).^2;
% The gradient should be zero (within floating point noise)
[grad,err] = gradest(rosen,[1 1])

%% The Hessian matrix at the minimizer should be positive definite
H = hessian(rosen,[1 1])
% The eigenvalues of h should be positive
eig(H)

%% Gradient estimation using gradest - a function of 5 variables
[grad,err] = gradest(@(x) sum(x.^2),[1 2 3 4 5])

%% Simple Hessian matrix of a problem with 3 independent variables
[H,err] = hessian(@(x) x(1) + x(2)^2 + x(3)^3,[1 2 3])

%% A semi-definite Hessian matrix
H = hessian(@(xy) cos(xy(1) - xy(2)),[0 0])
% one of these eigenvalues will be zero (approximately)
eig(H)

%% Directional derivative of the Rosenbrock function at the solution
% This should be zero. Ok, its a trivial test case.
[dd,err] = directionaldiff(rosen,[1 1],[1 2])

%% Directional derivative at other locations
[dd,err] = directionaldiff(rosen,[2 3],[1 -1])

% We can test this example
v = [1 -1];
v = v/norm(v);
g = gradest(rosen,[2 3]);

% The directional derivative will be the dot product of the gradient with
% the (unit normalized) vector. So this difference will be (approx) zero.
dot(g,v) - dd

%% Jacobian matrix of a scalar function is just the gradient
[jac,err] = jacobianest(rosen,[2 3])

grad = gradest(rosen,[2 3])

%% Jacobian matrix of a linear system will reduce to the design matrix
A = rand(5,3);
b = rand(5,1);
fun = @(x) (A*x-b);

x = rand(3,1);
[jac,err] = jacobianest(fun,x)

disp 'This should be essentially zero at any location x'
jac - A

%% The jacobian matrix of a nonlinear transformation of variables
% evaluated at some arbitrary location [-2, -3]
fun = @(xy) [xy(1).^2, cos(xy(1) - xy(2))];
[jac,err] = jacobianest(fun,[-2 -3])



##### SOURCE END #####
-->
   </body>
</html>